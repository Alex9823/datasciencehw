{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "DSassignment9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuMyevR2w0Fa",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 9\n",
        "\n",
        "Use data from `https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip`  \n",
        "Implement model in pytorch from [\"An Unsupervised Neural Attention Model for Aspect Extraction, He et al, 2017\"](https://www.comp.nus.edu.sg/~leews/publications/acl17.pdf), also desribed in seminar notes.  \n",
        "\n",
        "\n",
        "You can use sentence embeddings with attention **[7 points]**:  \n",
        "$z_s = \\sum_{i}^n \\alpha_i e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$\\alpha_i = softmax(d_i)$  attention weight for i-th token  \n",
        "$d_i = e_{w_i}^T M y_s$ attention with trainable matrix $M \\in R^{dxd}$  \n",
        "$y_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, y_s \\in R^d$ sentence context  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        "\n",
        "**Or** just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
        "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
        "$e_{w_i} \\in R^d$, token embedding of size d  \n",
        "$n$ - number of tokens in a sentence  \n",
        " \n",
        "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
        "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
        "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n",
        "\n",
        "\n",
        "**Training objective**:\n",
        "$$ J = \\sum_{s \\in D} \\sum_{i=1}^m max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
        "where   \n",
        "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
        "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
        "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
        "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm\n",
        "\n",
        "\n",
        "**[3 points]** Compute topic coherence for at least for 3 different number of topics. Use 10 nearest words for each topic. It means you have to train one model for each number of topics. You can use code from seminar notes with word2vec similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czt2JCSVw0Fe",
        "colab_type": "code",
        "outputId": "c815f94b-1201-42e9-d2a5-ea674920275a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget -O data.zip https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-22 20:43:11--  https://github.com/thedenaas/hse_seminars/blob/master/2018/seminar_13/data.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-22 20:43:11--  https://github.com/thedenaas/hse_seminars/raw/master/2018/seminar_13/data.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip [following]\n",
            "--2020-03-22 20:43:12--  https://raw.githubusercontent.com/thedenaas/hse_seminars/master/2018/seminar_13/data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9927168 (9.5M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   9.47M  50.8MB/s    in 0.2s    \n",
            "\n",
            "2020-03-22 20:43:12 (50.8 MB/s) - ‘data.zip’ saved [9927168/9927168]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2F0o7yqaBfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.data import Field, TabularDataset, Iterator\n",
        "from itertools import combinations\n",
        "from scipy.ndimage.filters import gaussian_filter1d\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from nltk import tokenize\n",
        "import nltk\n",
        "from zipfile import ZipFile\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import pickle\n",
        "import random\n",
        "from itertools import combinations\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdIKwB4_vYbN",
        "colab_type": "code",
        "outputId": "865fefb7-00f9-42b8-941b-6d678e7f8fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-22 20:43:15--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-22 20:43:15--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-22 20:43:15--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.3’\n",
            "\n",
            "glove.6B.zip.3      100%[===================>] 822.24M  2.08MB/s    in 6m 28s  \n",
            "\n",
            "2020-03-22 20:49:44 (2.12 MB/s) - ‘glove.6B.zip.3’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.50d.txt        \n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.100d.txt       \n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: aa\n",
            "error:  invalid response [aa]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: glove.6B.200d.txt       \n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: \n",
            "error:  invalid response [{ENTER}]\n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4P3jPR6yU_N",
        "colab_type": "code",
        "outputId": "0d4e5bda-fc2d-4f17-eeb3-832ac71949de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace data.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace stopwords.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2U8FL9reqF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF8zkTfhesri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_seed(value):\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puoth8LzC36v",
        "colab_type": "code",
        "outputId": "2c20bf37-0008-4955-9c94-3389196f72ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar 22 20:55:48 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    29W / 149W |     11MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZUvAeyWv8Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OaviX4Lv8RN",
        "colab_type": "code",
        "outputId": "72a8be25-8813-4309-9107-51b266e51c63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "if not Path('emb_word2vec_format.txt').exists():\n",
        "    glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"emb_word2vec_format.txt\")\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('emb_word2vec_format.txt')\n",
        "weights = torch.FloatTensor(model.vectors)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7hDEDDDv8Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx = {k:i for i, k in enumerate(model.vocab.keys())}\n",
        "weight = np.array([model[k] for _, k in enumerate(model.vocab.keys())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydXo8gIqv9Z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc0HJ1xx1Fqy",
        "colab_type": "text"
      },
      "source": [
        "**Подготовим данные**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdHPB3x4yhkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_NNHgfxo5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = []\n",
        "with open( \"stopwords.txt\", \"r\" ) as f:\n",
        "    for line in f.readlines():\n",
        "        stopwords.append( line.strip().lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0abdv3ysVMlT",
        "colab_type": "code",
        "outputId": "aeed565b-c329-4fcc-98b9-939a3be7811a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0oIOI0W0wUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_documents = []\n",
        "snippets = []\n",
        "with open( \"data.txt\", \"r\") as f:\n",
        "    for line in f.readlines():\n",
        "        text = line.strip()\n",
        "        raw_documents.append( text.lower() )\n",
        "        \n",
        "        snippets.append( text[0:min(len(text),100)] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V97gWI-5vuEB",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOakIDzcwhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSampler():\n",
        "    def __init__(self, dataset, negative_size=10, batch_size=5):\n",
        "        self.dataset = random.sample(dataset, len(dataset))\n",
        "        self.negative_size = negative_size\n",
        "        self.batch_size = batch_size\n",
        "        self.cur_idx = 0\n",
        "        self.seed = 0 \n",
        "\n",
        "    def __next__(self):\n",
        "        batch = {'positive': [], 'negative': []}\n",
        "        for j in range(self.batch_size):\n",
        "            positive = self.dataset[self.cur_idx]\n",
        "            rest = self.dataset[:self.batch_size*self.cur_idx+j] + self.dataset[self.batch_size*self.cur_idx+j+1:]\n",
        "            random.seed(self.seed + j)\n",
        "            negative = torch.stack(random.sample(rest, self.negative_size), 0)\n",
        "            batch['positive'].append(positive)\n",
        "            batch['negative'].append(negative)\n",
        "        batch['positive'] = torch.stack(batch['positive'], 0)\n",
        "        batch['negative'] = torch.stack(batch['negative'], 0)\n",
        "          \n",
        "        return batch\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.cur_idx = 0\n",
        "        for i in range(len(self.dataset) // self.batch_size):\n",
        "            self.cur_idx += 1\n",
        "            self.seed = i * self.batch_size\n",
        "            yield self.__next__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9BM26arzJqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padding_idx = word2idx['pad']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ0graUCz2Es",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "dataset = [tokenizer.tokenize(x) for x in raw_documents]\n",
        "\n",
        "if not Path('vocab.pickle').exists():\n",
        "    vocab_freq = {}\n",
        "    for doc in dataset:\n",
        "        for word in doc:\n",
        "            if word in vocab_freq:\n",
        "                vocab_freq[word] += 1\n",
        "            else:\n",
        "                vocab_freq[word] = 1\n",
        "else:\n",
        "    with open('vocab.pickle', 'rb') as f:\n",
        "        vocab_freq = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J3gHgVt0_qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_list = [k for k, v in vocab_freq.items() if v > 50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pov3yNVze8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "encode = lambda x: word2idx[x] if x in model.vocab.keys() else word2idx['unk']\n",
        "dataset = [[encode(x) for x in y] for y in dataset]\n",
        "dataset = dataset + [[encode(x)] for x in vocab_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f_aB4-VzYIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 512\n",
        "tensor_dataset = []\n",
        "for doc in dataset:\n",
        "    tensor_dataset.append(torch.LongTensor(doc[:max_len]+(max_len - len(doc))*[encode('pad')]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69FdgPbieCJ1",
        "colab_type": "text"
      },
      "source": [
        "**Модель**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7FgHK6scwn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TopicModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d, n_topics):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d = d\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d)\n",
        "        self.M_matrix = nn.Linear(d, d, bias=False)\n",
        "        self.proj = nn.Linear(d, n_topics)\n",
        "\n",
        "        self.T_matrix = nn.Parameter(nn.init.xavier_uniform_(torch.empty(n_topics, d)))\n",
        "\n",
        "    def load_embedding_weight(self, weight, padding_idx=None, freeze=False):\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.d).from_pretrained(weight, padding_idx=padding_idx)\n",
        "        if freeze == True:\n",
        "            self.embedding.requires_grad = False\n",
        "\n",
        "    def forward(self, batch):\n",
        "        pos_emb = self.embedding(batch['positive'])\n",
        "        neg_context_emb = self.embedding(batch['negative']).mean(2)\n",
        "\n",
        "        sent_context = pos_emb.mean(1)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('ble,be->bl', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('ble,bl->be', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn), -1)\n",
        "        r = p @ self.T_matrix\n",
        "        \n",
        "        pos = torch.einsum('be,be->b', r, attn)\n",
        "        neg = torch.einsum('be,bme->bm', r, neg_context_emb)\n",
        "\n",
        "        return pos, neg \n",
        "    \n",
        "    def get_probs(self, inp):\n",
        "        pos_emb = self.embedding(inp)\n",
        "\n",
        "        sent_context = pos_emb.mean(0)\n",
        "        transf_emb = self.M_matrix(pos_emb)\n",
        "        sim = torch.einsum('le,e->l', transf_emb, sent_context)\n",
        "        alphas = F.softmax(sim, -1)\n",
        "        attn = torch.einsum('le,l->e', pos_emb, alphas)\n",
        "        p = F.softmax(self.proj(attn))\n",
        "\n",
        "        return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijU0XsrYcwvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epoch = 5\n",
        "batch_size = 10\n",
        "negative_size = 20\n",
        "lamda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkx-4RK7cw1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_topics_range = [3, 4, 5, 6, 7, 8, 9, 10]\n",
        "topic_models = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSEqOUdJeVX2",
        "colab_type": "code",
        "outputId": "de44b9bf-4242-46c0-e0a2-6679adf23db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for n_topics in n_topics_range:\n",
        "    print(f'{n_topics} topics \\n')\n",
        "    random_seed(999)\n",
        "    topic_model = TopicModel(len(word2idx), 300, n_topics=n_topics)\n",
        "    topic_model.load_embedding_weight(torch.FloatTensor(weight), padding_idx=padding_idx, freeze=True)\n",
        "    topic_model.to(device)\n",
        "    optimizer = Adam(topic_model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
        "\n",
        "    def topic_loss_function(pos, neg, model):\n",
        "        pos = pos.unsqueeze(-1)\n",
        "        delta = 1 - pos + neg\n",
        "        delta = F.relu(delta)\n",
        "        reg = torch.frobenius_norm(model.T_matrix @ model.T_matrix.permute(1,0)  - torch.eye(n_topics).to(pos.device))\n",
        "        loss = delta.sum() / batch_size + lamda * reg / len(sampler) / batch_size\n",
        "        return loss\n",
        "\n",
        "    topic_model.train()\n",
        "    for ep in range(n_epoch):\n",
        "        ep_loss = 0\n",
        "        sampler = DataSampler(tensor_dataset, negative_size=negative_size, batch_size=batch_size)\n",
        "        for step, batch in enumerate(iter(sampler)):\n",
        "            for k, v in batch.items():\n",
        "                batch[k] = v.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pos, neg = topic_model(batch)\n",
        "            loss = topic_loss_function(pos, neg, topic_model)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ep_loss += loss.item()\n",
        "        scheduler.step(ep_loss)\n",
        "        print(f'Epoch {ep}, loss {ep_loss / len(sampler)}')\n",
        "\n",
        "    topic_model.eval()\n",
        "    with torch.no_grad():\n",
        "        W = []\n",
        "        for x in tensor_dataset:\n",
        "            W.append(topic_model.get_probs(x.to(device)).cpu().numpy())\n",
        "        W = np.array(W)\n",
        "\n",
        "        H = []\n",
        "        for x in vocab_list:\n",
        "            if x not in word2idx.keys():\n",
        "                continue\n",
        "            H.append(topic_model.get_probs(torch.tensor([word2idx[x]]).to(device)).cpu().numpy())\n",
        "        H = np.array(H)\n",
        "        H = H.transpose()\n",
        "\n",
        "    topic_models.append((n_topics, W, H))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics \n",
            "\n",
            "Epoch 0, loss 1.9149345496476227\n",
            "Epoch 1, loss 0.5372674344885393\n",
            "Epoch 2, loss 0.23591477085950507\n",
            "Epoch 3, loss 0.13628685431837448\n",
            "Epoch 4, loss 0.07475079432465356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4 topics \n",
            "\n",
            "Epoch 0, loss 1.950862026236933\n",
            "Epoch 1, loss 0.5755815415883574\n",
            "Epoch 2, loss 0.2423686529863145\n",
            "Epoch 3, loss 0.10766235287396982\n",
            "Epoch 4, loss 0.04486044019254736\n",
            "5 topics \n",
            "\n",
            "Epoch 0, loss 2.08254938715119\n",
            "Epoch 1, loss 0.5563474206519411\n",
            "Epoch 2, loss 0.22361525588479236\n",
            "Epoch 3, loss 0.10516403289501615\n",
            "Epoch 4, loss 0.048700271897553285\n",
            "6 topics \n",
            "\n",
            "Epoch 0, loss 2.066286248204463\n",
            "Epoch 1, loss 0.4534000423928859\n",
            "Epoch 2, loss 0.15305404735768616\n",
            "Epoch 3, loss 0.05572209231965309\n",
            "Epoch 4, loss 0.02025284052827279\n",
            "7 topics \n",
            "\n",
            "Epoch 0, loss 2.2445070831384464\n",
            "Epoch 1, loss 0.5039599054086727\n",
            "Epoch 2, loss 0.1735416554600613\n",
            "Epoch 3, loss 0.07447927892491245\n",
            "Epoch 4, loss 0.03060126555922636\n",
            "8 topics \n",
            "\n",
            "Epoch 0, loss 2.099631497687388\n",
            "Epoch 1, loss 0.39927460073630106\n",
            "Epoch 2, loss 0.13011391842998532\n",
            "Epoch 3, loss 0.04307928825706188\n",
            "Epoch 4, loss 0.010784386056463119\n",
            "9 topics \n",
            "\n",
            "Epoch 0, loss 7.771446455326019\n",
            "Epoch 1, loss 7.820559848397809\n",
            "Epoch 2, loss 0.6949806072480719\n",
            "Epoch 3, loss 0.1366025963104562\n",
            "Epoch 4, loss 0.05653767970314026\n",
            "10 topics \n",
            "\n",
            "Epoch 0, loss 8.340949622161416\n",
            "Epoch 1, loss 8.217614013860182\n",
            "Epoch 2, loss 1.31543687481524\n",
            "Epoch 3, loss 0.17936875411367967\n",
            "Epoch 4, loss 0.07260847733069246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R7eKInCeVap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fhB4SHP1rvM",
        "colab_type": "text"
      },
      "source": [
        "**Coherence**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8A0WCYCeVdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terms = list(filter(lambda x: x in word2idx.keys(), vocab_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Wbtbt5eVfu",
        "colab_type": "code",
        "outputId": "1c030361-ad49-43ca-8fe4-1865df7a191d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def get_descriptor( terms, H, topic_index, top ):\n",
        "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
        "    top_terms = []\n",
        "    for term_index in top_indices[0:top]:\n",
        "        top_terms.append( terms[term_index] )\n",
        "    return top_terms\n",
        "\n",
        "for (k,_,H) in topic_models:\n",
        "    print(f'{k} topics')\n",
        "    descriptors = []\n",
        "    for topic_index in range(k):\n",
        "        descriptors.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "        str_descriptor = \", \".join( descriptors[topic_index] )\n",
        "        print(\"Topic %02d: %s\" % ( topic_index+1, str_descriptor ) )\n",
        "    print('\\n')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 topics\n",
            "Topic 01: reforms, pledged, ministers, allies, leaders, tensions, policymakers, economic, austerity, confidence\n",
            "Topic 02: crying, laughing, screaming, tears, guy, loud, heard, dressed, surrounded, wearing\n",
            "Topic 03: download, multiple, wi-fi, format, #, species, user, server, entries, 2013\n",
            "\n",
            "\n",
            "4 topics\n",
            "Topic 01: percent, ministers, against, parliamentary, votes, elections, opposition, parliament, election, presidency\n",
            "Topic 02: please, information, data, download, identify, customer, providers, disclose, publish, users\n",
            "Topic 03: cech, klopp, grimsby, belfast, paschi, wembley, organised, reporters, breakfast, davos\n",
            "Topic 04: funny, comedy, boring, mouth, piano, kid, weird, fiction, guitar, discovered\n",
            "\n",
            "\n",
            "5 topics\n",
            "Topic 01: productivity, devices, applications, systems, properties, larger, smaller, function, offset, products\n",
            "Topic 02: somebody, sorry, anybody, yeah, scared, me, everybody, ok, nobody, guess\n",
            "Topic 03: award, fame, awards, acclaimed, prize, favourite, tribute, guitarist, jazz, artist\n",
            "Topic 04: meeting, council, talks, minister, ministers, un, secretary, conference, deputy, committee\n",
            "Topic 05: phrase, mocked, spoken, chant, willian, refer, obituary, language, commentators, sung\n",
            "\n",
            "\n",
            "6 topics\n",
            "Topic 01: daughter, son, wife, father, friend, sir, brother, mother, elizabeth, sarah\n",
            "Topic 02: campaigner, terrifying, misogyny, sexist, ultimate, noise, denies, cruel, unbelievable, evil\n",
            "Topic 03: #, 0-1, g, singles, hits, 2013, b, w, 2012, f\n",
            "Topic 04: housing, buildings, agricultural, infrastructure, industrial, construction, vast, large, manufacturing, incomes\n",
            "Topic 05: long., fca, literature, film-makers, giroud, literary, recognised, lgbt, far-right, ’\n",
            "Topic 06: elections, referendum, will, agreement, resolution, vote, iraq, pledge, pledged, mandate\n",
            "\n",
            "\n",
            "7 topics\n",
            "Topic 01: professor, attorney, lawyer, assistant, married, appointed, margaret, associate, studied, edward\n",
            "Topic 02: songs, album, dark, song, lyrics, pop, tunes, sounds, beautiful, soundtrack\n",
            "Topic 03: grimsby, village, town, burnley, deeney, bromwich, whitehall, free-kick, brighton, watford\n",
            "Topic 04: dems, hacked, passwords, comfortably, waited, shoulder, sit, routinely, caucus, terrified\n",
            "Topic 05: consecutive, percent, total, june, 13, 22, 14, july, 19, march\n",
            "Topic 06: we, determination, respect, our, want, need, urgency, realise, willingness, ourselves\n",
            "Topic 07: guidolin, tillerson, lamela, pardew, 5., obr, klopp, cryan, pro-eu, said.\n",
            "\n",
            "\n",
            "8 topics\n",
            "Topic 01: cinematic, toxic, bubble, misogyny, old-fashioned, taste, pure, sexism, surreal, symbol\n",
            "Topic 02: we, do, n't, are, them, they, i, think, know, have\n",
            "Topic 03: 2016, 2018, everton, cup, appoint, afc, striker, trafford, 2-1, anfield\n",
            "Topic 04: sir, became, dark, gray, grey, baron, becomes, wood, brown, hall\n",
            "Topic 05: applications, (, f, l, m, ), mobile, technologies, provider, c\n",
            "Topic 06: copy, reads, adele, emails, download, hello, privacy, blog, youtube, sex\n",
            "Topic 07: economic, ongoing, amid, relations, cooperation, reforms, billion, western, union, citing\n",
            "Topic 08: cap, removing, states., bony, fines, fellaini, wijnaldum, koscielny, first-half, hinkley\n",
            "\n",
            "\n",
            "9 topics\n",
            "Topic 01: jp, chairman, bloomberg, healthcare, reuters, hsbc, commissioner, equity, takeover, dow\n",
            "Topic 02: season, 6, 8, 12, 14, 9, 7, 4, 3, seasons\n",
            "Topic 03: rapper, oscar, lennon, hello, pep, screenplay, adele, birthday, comedian, cheers\n",
            "Topic 04: puncheon, yoga, koscielny, toilet, bruyne, lallana, bony, wan, willian, finger\n",
            "Topic 05: terrifying, emotionally, yellen, obsession, utterly, long., awakens, grim, bleak, firmino\n",
            "Topic 06: acoustic, plastic, clothes, flowers, hair, stones, instruments, bands, landscape, camera\n",
            "Topic 07: we, necessary, implement, intend, should, undermine, reduce, encourage, intent, any\n",
            "Topic 08: paschi, darmian, bma, benghazi, westpac, fca, wijnaldum, councils, clinics, group.\n",
            "Topic 09: doesn, shouldn, didn, wasn, isn, !, q, gon, ll, wouldn\n",
            "\n",
            "\n",
            "10 topics\n",
            "Topic 01: university, at, world, two, four, major, three, -, american, international\n",
            "Topic 02: senator, denied, deny, lawyer, attorney, democrat, ali, refused, denies, convince\n",
            "Topic 03: typically, usually, flowers, user, color, thick, function, medium, distinctive, typical\n",
            "Topic 04: policymakers, implement, comply, sanctions, guidelines, reforms, opec, austerity, resolved, regulations\n",
            "Topic 05: paschi, said., shouldn, obr, hasn, darmian, so., mazzarri, 1bn, lamela\n",
            "Topic 06: we, lot, i, going, guys, you, ride, fun, real, just\n",
            "Topic 07: tusk, |, yoga, trauma, wan, dem, lib, unite, rr, thinktank\n",
            "Topic 08: darmian, lukaku, kanté, payet, mignolet, pogba, pickford, lallana, gueye, bruyne\n",
            "Topic 09: spotify, hashtag, 2., 5., hacking, buzzfeed, searches, follow-up, carers, benghazi\n",
            "Topic 10: free-kick, sturridge, lloris, manafort, zika, smalling, mignolet, kick-off, millennials, lallana\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTVg9-RHeVlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_coherence( w2v_model, term_rankings ):\n",
        "    overall_coherence = 0.0\n",
        "    for topic_index in range(len(term_rankings)):\n",
        "        # check each pair of terms\n",
        "        pair_scores = []\n",
        "        for pair in combinations( term_rankings[topic_index], 2 ):\n",
        "            if pair[0] in w2v_model.vocab and pair[1] in w2v_model.vocab:\n",
        "                pair_scores.append( w2v_model.similarity(pair[0], pair[1]) )\n",
        "        # get the mean for all pairs in this topic\n",
        "        topic_score = sum(pair_scores) / len(pair_scores)\n",
        "        overall_coherence += topic_score\n",
        "    # get the mean score across all topics\n",
        "    return overall_coherence / len(term_rankings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYy1u9dmeVnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oS3j0Hi3ChH",
        "colab_type": "text"
      },
      "source": [
        "****Topic Coherence****\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8C6R3deVqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "6c4654f9-9a46-4c06-ad0c-1895d5a97cb4"
      },
      "source": [
        "k_values = []\n",
        "coherences = []\n",
        "for (k,W,H) in topic_models:\n",
        "    term_rankings = []\n",
        "    for topic_index in range(k):\n",
        "        term_rankings.append( get_descriptor( terms, H, topic_index, 10 ) )\n",
        "    k_values.append( k )\n",
        "    coherences.append( calculate_coherence( model, term_rankings ) )\n",
        "    print(\"K=%02d: Coherence=%.4f\" % ( k, coherences[-1] ) )"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K=03: Coherence=0.3056\n",
            "K=04: Coherence=0.2673\n",
            "K=05: Coherence=0.3684\n",
            "K=06: Coherence=0.3009\n",
            "K=07: Coherence=0.3372\n",
            "K=08: Coherence=0.2787\n",
            "K=09: Coherence=0.2443\n",
            "K=10: Coherence=0.2486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}